{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257f9c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for nltk...\n",
      "\n",
      "Checking for numpy...\n",
      "\n",
      "Checking for pandas...\n",
      "\n",
      "Checking for scipy...\n",
      "\n",
      "Checking for sklearn...\n",
      "\n",
      "Checking for pickle...\n",
      "\n",
      "Checking for re...\n",
      "\n",
      "System is ready!\n",
      "Model File exist\n",
      "Vectorizer File exist\n",
      "Downloading Enron emails in the Downloads folder...\n",
      "Download, unzip, and save to pickle done!\n",
      "C:\\Users\\User/Downloads\n",
      "RangeIndex(start=0, stop=33716, step=1)\n",
      "Number of different words: 119405\n",
      "Word example: arcadian\n",
      "(26972, 119405) (26972,)\n",
      "(6744, 119405) (6744,)\n",
      "Accuracy: 0.9847271648873073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "dependencies = [\"nltk\", \"numpy\", \"pandas\", \"scipy\", \"sklearn\", \"pickle\", \"re\"]\n",
    "\n",
    "for module in dependencies:\n",
    "    print(\"\\nChecking for \" + module + \"...\")\n",
    "    try:\n",
    "        # Import module from string variable:\n",
    "        module_obj = __import__(module)\n",
    "\n",
    "        # To contain the module, create a global object using globals()\n",
    "        globals()[module] = module_obj\n",
    "    except ImportError:\n",
    "        print(\"Install \" + module + \" before continuing\")\n",
    "        print(\"In a terminal type the following commands:\")\n",
    "        print(\"python get-pip.py\")\n",
    "        print(\"pip install \" + module + \"\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "print(\"\\nSystem is ready!\")\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import os.path\n",
    "\n",
    "if os.path.isfile('trained_model.pickle'):\n",
    "    print (\"Model File exist\")\n",
    "else:\n",
    "    createfile = open('trained_model.pickle', 'w+')\n",
    "    \n",
    "if os.path.isfile('vectorizer.pickle'):\n",
    "    print (\"Vectorizer File exist\")\n",
    "else:\n",
    "    createfile = open('vectorizer.pickle', 'w+')\n",
    "\n",
    "print(\"Downloading Enron emails in the Downloads folder...\")\n",
    "\n",
    "# Get the user's Downloads folder path\n",
    "downloads = os.path.join(os.path.expanduser('~') + \"/Downloads\")\n",
    "\n",
    "url = \"http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/\"\n",
    "\n",
    "enron_dir = os.path.join(downloads, 'Enron emails')\n",
    "\n",
    "enron_files = ['enron1.tar.gz', 'enron2.tar.gz', 'enron3.tar.gz',\n",
    "               'enron4.tar.gz', 'enron5.tar.gz', 'enron6.tar.gz']\n",
    "\n",
    "def download():\n",
    "    \"\"\" Download Enron emails if missing. \"\"\"\n",
    "\n",
    "    # Create the directories.\n",
    "    if not os.path.exists(enron_dir):\n",
    "        os.makedirs(enron_dir)\n",
    "    # Download the files that not exist.\n",
    "    for file in enron_files:\n",
    "        path = os.path.join(enron_dir, file)\n",
    "        if not os.path.exists(path):\n",
    "            urllib.request.urlretrieve(url + file, path)\n",
    "\n",
    "def extract_emails(fname):\n",
    "    \"\"\" Extract the zipped emails and load them into a pandas df.\n",
    "    Args:\n",
    "        fname (str): the files with tar.gz extension\n",
    "    Returns:\n",
    "        pandas df: a pandas dataframe of emails\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "    tfile = tarfile.open(fname, 'r:gz')\n",
    "    for member in tfile.getmembers():\n",
    "        if 'ham' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'message': row, 'class': 'ham'})\n",
    "        if 'spam' in member.name:\n",
    "            f = tfile.extractfile(member)\n",
    "            if f is not None:\n",
    "                row = f.read()\n",
    "                rows.append({'message': row, 'class': 'spam'})\n",
    "    tfile.close()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def populate_df_and_pickle():\n",
    "    \"\"\" Populate the df with all the emails and save it to a pickle object. \"\"\"\n",
    "\n",
    "    if not os.path.exists(downloads + \"/emails.pickle\"):\n",
    "        emails_df = pd.DataFrame({'message': [], 'class': []})\n",
    "        for file in enron_files:\n",
    "            unzipped_file = extract_emails(os.path.join(enron_dir, file))\n",
    "            emails_df = emails_df.append(unzipped_file)\n",
    "        emails_df.to_pickle(downloads + \"/emails.pickle\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    download()\n",
    "    populate_df_and_pickle()\n",
    "    print(\"Download, unzip, and save to pickle done!\")\n",
    "\n",
    "\n",
    "print(downloads)\n",
    "\n",
    "with open(downloads + '/emails.pickle', 'rb') as f:\n",
    "    emails_df = pickle.load(f) \n",
    "\n",
    "# Translate bytes objects into strings.\n",
    "emails_df['message'] = emails_df['message'].apply(lambda x: x.decode('latin-1'))\n",
    "\n",
    "# Reset pandas df index.\n",
    "emails_df = emails_df.reset_index(drop=True)\n",
    "\n",
    "# Map 'spam' to 1 and 'ham' to 0.\n",
    "emails_df['class'] = emails_df['class'].map({'spam':1, 'ham':0})\n",
    "\n",
    "print(emails_df.index)\n",
    "emails_df.shape\n",
    "\n",
    "emails_df.iloc[25000].values\n",
    "\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def clean_email(email):\n",
    "    \"\"\" Remove all punctuation, urls, numbers, and newlines.\n",
    "    Convert to lower case.\n",
    "    Args:\n",
    "        email (unicode): the email\n",
    "    Returns:\n",
    "        email (unicode): only the text of the email\n",
    "    \"\"\"\n",
    "\n",
    "    email = re.sub(r'http\\S+', ' ', email)\n",
    "    email = re.sub(\"\\d+\", \" \", email)\n",
    "    email = email.replace('\\n', ' ')\n",
    "    email = email.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    email = email.lower()\n",
    "    return email\n",
    "\n",
    "emails_df['message'] = emails_df['message'].apply(clean_email)\n",
    "\n",
    "emails_df.iloc[25000].values\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# nltk.download('wordnet') # uncomment to download 'wordnet'\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def preproces_text(email):\n",
    "    \"\"\" Split the text string into individual words, stem each word,\n",
    "    and append the stemmed word to words. Make sure there's a single\n",
    "    space between each stemmed word.\n",
    "    Args:\n",
    "        email (unicode): the email\n",
    "    Returns:\n",
    "        words (unicode): the text of the email\n",
    "    \"\"\"\n",
    "\n",
    "    words = \"\"\n",
    "    # Create the stemmer.\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # Split text into words.\n",
    "    email = email.split()\n",
    "    for word in email:\n",
    "        # Optional: remove unknown words.\n",
    "        # if wn.synsets(word):\n",
    "        words = words + stemmer.stem(word) + \" \"\n",
    "\n",
    "    return words\n",
    "\n",
    "emails_df['message'] = emails_df['message'].apply(preproces_text)\n",
    "\n",
    "emails_df.iloc[25000].values\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the independent variables as Xs.\n",
    "Xs = emails_df['message'].values\n",
    "\n",
    "# Define the target (dependent) variable as Ys.\n",
    "Ys = emails_df['class'].values\n",
    "\n",
    "# Vectorize words - Turn the text numerical feature vectors,\n",
    "# using the strategy of tokenization, counting and normalization.\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                       stop_words='english')\n",
    "\n",
    "Xs = vectorizer.fit_transform(Xs)\n",
    "\n",
    "# Create a train/test split using 20% test size.\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs,\n",
    "                                                    Ys,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Ys)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(\"Number of different words: {0}\".format(len(feature_names)))\n",
    "print(\"Word example: {0}\".format(feature_names[5369]))\n",
    "\n",
    "# Check the split printing the shape of each set.\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create classifier.\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier on the training features and labels.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "import pickle\n",
    "with open(\"trained_model.pickle\", \"wb\") as file:\n",
    "    pickle.dump(clf, file)\n",
    "with open(\"vectorizer.pickle\", \"wb\") as file2:\n",
    "    pickle.dump(vectorizer, file2)\n",
    "\n",
    "# Calculate the accuracy on the test data.\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abef898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
